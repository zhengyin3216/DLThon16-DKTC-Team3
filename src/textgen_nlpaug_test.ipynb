{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1c51a7-6692-433c-8d7c-4cb1c0763b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (1.1.11)\n",
      "Requirement already satisfied: konlpy in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: mecab-python in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from nlpaug) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from nlpaug) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from nlpaug) (2.32.5)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from nlpaug) (5.2.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: mecab-python3 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from mecab-python) (1.0.12)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (4.14.2)\n",
      "Requirement already satisfied: filelock in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (3.20.3)\n",
      "Requirement already satisfied: tqdm in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (4.67.2)\n",
      "Requirement already satisfied: packaging in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug konlpy mecab-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5c862-63f1-485b-8fc3-2546a232265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jjeong3150/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cf38e3-b754-4e71-b3de-c1e75f600e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 너 스스로를 죽여달라고 애원하는 것인가? 아닙니다. 죄송합니다. 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게. 정말 잘못했습니다. 너가 선택해. 너가 죽을래 네 가족을 죽여줄까. 죄송합니다. 정말 잘못했습니다. 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야. 선택 못하겠습니다. 한번만 도와주세요. 그냥 다 죽여버려야겠군. 이의 없지? 제발 도와주세요.\n"
     ]
    }
   ],
   "source": [
    "text = \"지금 너 스스로를 죽여달라고 애원하는 것인가? 아닙니다. 죄송합니다. 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게. 정말 잘못했습니다. 너가 선택해. 너가 죽을래 네 가족을 죽여줄까. 죄송합니다. 정말 잘못했습니다. 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야. 선택 못하겠습니다. 한번만 도와주세요. 그냥 다 죽여버려야겠군. 이의 없지? 제발 도와주세요.\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b091d29-3303-429b-ad63-10fb1430dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지금 너 스스로를 죽여달라고 애원하는 것인가? 아닙니다. 죄송합니다. 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게. 정말 잘못했습니다. 너가 선택해. 너가 죽을래 네 가족을 죽여줄까. 죄송합니다. 정말 잘못했습니다. 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야. 선택 못하겠습니다. 한번만 도와주세요. 그냥 다 죽여버려야겠군. 이의 없지? 제발 도와주세요.']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "aug_text = aug.augment(text)\n",
    "\n",
    "print(aug_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7465551f-db4d-44e9-bf72-ddd8c0b58a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/augmenter/word/context_word_embs.py:123: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  prefix_reg = '(?<=\\s|\\W)'\n",
      "/home/jjeong3150/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/augmenter/word/context_word_embs.py:124: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  suffix_reg = '(?=\\s|\\W)'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "BertTokenizer has no attribute _convert_token_to_id",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlpaug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmenter\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mword\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnaw\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m aug = \u001b[43mnaw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContextualWordEmbsAug\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mklue/bert-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubstitute\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43maug_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(aug.augment(text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/augmenter/word/context_word_embs.py:98\u001b[39m, in \u001b[36mContextualWordEmbsAug.__init__\u001b[39m\u001b[34m(self, model_path, model_type, action, top_k, name, aug_min, aug_max, aug_p, stopwords, batch_size, device, force_reload, stopwords_regex, verbose, silence, use_custom_api)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# TODO: Slow when switching to HuggingFace pipeline. #https://github.com/makcedward/nlpaug/issues/248\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.use_custom_api = use_custom_api\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_custom_api\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_custom_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Override stopwords\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# if stopwords and self.model_type in ['xlnet', 'roberta']:\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#     stopwords = [self.stopwords]\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# lower case all stopwords\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stopwords \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33muncased\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_path:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/augmenter/word/context_word_embs.py:533\u001b[39m, in \u001b[36mContextualWordEmbsAug.get_model\u001b[39m\u001b[34m(cls, model_path, model_type, device, force_reload, batch_size, top_k, silence, use_custom_api)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path, model_type, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, force_reload=\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m    532\u001b[39m     top_k=\u001b[38;5;28;01mNone\u001b[39;00m, silence=\u001b[38;5;28;01mTrue\u001b[39;00m, use_custom_api=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_context_word_embs_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_custom_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/augmenter/word/context_word_embs.py:34\u001b[39m, in \u001b[36minit_context_word_embs_model\u001b[39m\u001b[34m(model_path, model_type, device, force_reload, batch_size, top_k, silence, use_custom_api)\u001b[39m\n\u001b[32m     32\u001b[39m     model = nml.Roberta(model_path, device=device, top_k=top_k, silence=silence, batch_size=batch_size)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_type == \u001b[33m'\u001b[39m\u001b[33mbert\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     model = \u001b[43mnml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mModel type value is unexpected. Only support bert and roberta models.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/model/lang_models/bert.py:34\u001b[39m, in \u001b[36mBert.__init__\u001b[39m\u001b[34m(self, model_path, temperature, top_k, top_p, batch_size, device, silence)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.model_path = model_path\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model_path)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mself\u001b[39m.mask_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken2id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mMASK_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.pad_id = \u001b[38;5;28mself\u001b[39m.token2id(\u001b[38;5;28mself\u001b[39m.PAD_TOKEN)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silence:\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Transformers thrown an warning regrading to weight initialization. It is expected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/nlpaug/model/lang_models/bert.py:61\u001b[39m, in \u001b[36mBert.token2id\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.convert_tokens_to_ids(token)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Old transformers API\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_convert_token_to_id\u001b[49m(token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1291\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens) \u001b[38;5;28;01mif\u001b[39;00m key != key_without_id \u001b[38;5;28;01melse\u001b[39;00m tokens\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1291\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattr__\u001b[39m(key)\n",
      "\u001b[31mAttributeError\u001b[39m: BertTokenizer has no attribute _convert_token_to_id"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"klue/bert-base\",\n",
    "    model_type=\"bert\",\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.2\n",
    ")\n",
    "\n",
    "print(aug.augment(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "989897ea-0e9d-4e27-9065-a94f0fe9f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['저기요 혹시 날이 너무 뜨겁잖f요? 저희 회V에서 이 선크림 [UNK] 한 번 손등에 발라보실2요? 아 진짜요? 안 그래도 선크림 필요해서 판매 중인데 한 번 발라 볼게요! 여기 한 번 볼까요. 진짜 성분도 좋고 다 좋아요. 하하. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않3봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가Z요. 네 많이 예T해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 쓴다고 해? 그럼 이거 누l한테 팔아요? 당신이 바른거를? 아니 조금 발라 보시라고 하셨잖아요. 먼저 권유@놓고 사라고 이거 갈취인K 몰라요? 내가 뭐 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 봐라 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나5']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "word_aug = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"klue/bert-base\",\n",
    "    model_type=\"bert\",\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.15\n",
    ")\n",
    "\n",
    "char_aug = nac.RandomCharAug(action=\"substitute\", aug_char_p=0.08)\n",
    "\n",
    "def augment_pipeline(text: str) -> str:\n",
    "    x = word_aug.augment(text)\n",
    "    x = char_aug.augment(x)\n",
    "    return x\n",
    "\n",
    "print(augment_pipeline(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73db540b-b26c-4a84-8d36-4e65aa173a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "\n",
    "# 1) Augmenters 준비 (한국어 대화용 추천 세팅)\n",
    "word_aug = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"klue/bert-base\",   # 한국어 BERT (MLM)\n",
    "    model_type=\"bert\",\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.4,                    # 단어 교체 비율 (0.10~0.20 추천)\n",
    ")\n",
    "\n",
    "char_aug = nac.RandomCharAug(\n",
    "    action=\"substitute\",\n",
    "    aug_char_p=0.06                # 문자 변형 비율 (0.03~0.08 추천)\n",
    ")\n",
    "\n",
    "swap_aug = naw.RandomWordAug(\n",
    "    action=\"swap\",\n",
    "    aug_p=0.1                     # 단어 스왑 비율 (0.03~0.10 추천)\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# A) 가장 흔한 방식: 순차 파이프라인 (의미보존 → 현실감)\n",
    "# -------------------------\n",
    "def augment_pipeline(text: str) -> str:\n",
    "    x = word_aug.augment(text)     # 문맥 기반 단어 교체\n",
    "    x = swap_aug.augment(x)        # 가벼운 오타/채팅 노이즈\n",
    "    return x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# B) 실무에서 많이 쓰는 방식: 확률 혼합 (한 문장에 하나만 적용)\n",
    "#   - 과변형 방지 + 다양성 확보\n",
    "# -------------------------\n",
    "def augment_mixture(text: str) -> str:\n",
    "    r = random.random()\n",
    "    if r < 0.55:\n",
    "        return word_aug.augment(text)   # 55%: 문맥 치환\n",
    "    elif r < 0.80:\n",
    "        return char_aug.augment(text)   # 25%: 문자 노이즈\n",
    "    else:\n",
    "        return swap_aug.augment(text)   # 20%: 단어 스왑 (약하게)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99416747-56ad-4013-aa75-d4e2898c7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요! 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네 ->\n",
      "['저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 전해 드릴까요? 아 진짜요? 안 그래도 하나 필요해서 알아보던 중인데 한 번 발라 볼게요 여기! 한 번 발라보세요. 진짜 다 성분도 다 좋아요. 음. 좋다고 성분이 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 진짜 이번에 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 파시고 많이 안녕히 계세요. 아니 저기요 돈 안내요? 네? 돈 발라보는것도 내야 한다? 아뇨 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 봐라 하셨잖아요. 먼저 이거 사라고 갈취인거 강매하는거 몰라요? 그렇게 내가 해도 된다고 말 한 적 있어? 그것도 모르고 바른걸 탓 누구 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 사람들한텐 다른 이렇게 갈취하지마세요. 화딱지나네']\n"
     ]
    }
   ],
   "source": [
    "print(text, \"->\")\n",
    "print(augment_pipeline(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiffel_learning_py312)",
   "language": "python",
   "name": "aiffel_learning_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
